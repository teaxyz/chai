#!/usr/bin/env pkgx +python@3.11 uv run --with psycopg2==2.9.9

import argparse
import csv
import os
import uuid
from datetime import datetime
from typing import Dict, List, Optional, Set, Tuple

import psycopg2
import psycopg2.extras

from core.config import Config, PackageManager
from core.logger import Logger

CHAI_DATABASE_URL = os.environ.get("CHAI_DATABASE_URL")
DEFAULT_BATCH_SIZE = 20000


class ChaiPackageUrlsDB:
    """Handles DB interactions for batch package_urls insertion."""

    def __init__(self, logger: Logger):
        self.logger = logger
        if not CHAI_DATABASE_URL:
            self.logger.error("CHAI_DATABASE_URL environment variable not set.")
            raise ValueError("CHAI_DATABASE_URL not set")
        self.conn = None
        self.cursor = None
        try:
            self.conn = psycopg2.connect(CHAI_DATABASE_URL)
            self.cursor = self.conn.cursor()
            self.logger.log("CHAI database connection established for PackageUrlsDB")
        except psycopg2.Error as e:
            self.logger.error(f"PackageUrlsDB connection error: {e}")
            raise

    def load_package_id_cache(self) -> Dict[str, uuid.UUID]:
        """Load all packages (import_id -> id) into a cache."""
        self.logger.log("Loading package_id cache from database...")
        query = "SELECT import_id, id FROM packages"
        try:
            self.cursor.execute(query)
            cache = {str(row[0]): row[1] for row in self.cursor.fetchall() if row[0]}
            self.logger.log(f"Loaded {len(cache)} packages into package_id cache.")
            return cache
        except psycopg2.Error as e:
            self.logger.error(f"Error loading package_id cache: {e}")
            raise

    def load_url_id_cache_from_db(
        self,
    ) -> Dict[Tuple[str, uuid.UUID], uuid.UUID]:
        """Load all URLs ( (url, url_type_id) -> id ) into a cache from DB."""
        self.logger.log("Loading url_id cache from database (fallback)...")
        query = "SELECT id, url, url_type_id FROM urls"
        cache: Dict[Tuple[str, uuid.UUID], uuid.UUID] = {}
        try:
            self.cursor.execute(query)
            for row in self.cursor.fetchall():
                url_id, url_str, url_type_id = row[0], row[1], row[2]
                if url_str and url_type_id:
                    cache[(url_str, url_type_id)] = url_id
            self.logger.log(f"Loaded {len(cache)} URLs into url_id cache from DB.")
            return cache
        except psycopg2.Error as e:
            self.logger.error(f"Error loading url_id cache from DB: {e}")
            raise

    def batch_insert_package_urls(
        self,
        data_tuples: List[Tuple[uuid.UUID, uuid.UUID, uuid.UUID, datetime, datetime]],
    ) -> None:
        """Batch insert into package_urls table."""
        if not data_tuples:
            return

        query = """
            INSERT INTO package_urls (id, package_id, url_id, created_at, updated_at)
            VALUES %s
            ON CONFLICT (package_id, url_id) 
            DO UPDATE SET updated_at = EXCLUDED.updated_at
        """
        try:
            psycopg2.extras.execute_values(
                self.cursor, query, data_tuples, page_size=len(data_tuples)
            )
            self.conn.commit()
            self.logger.log(
                f"Successfully inserted/updated {len(data_tuples)} package_urls"
            )
        except psycopg2.Error as e:
            self.logger.error(f"Error during batch insert into package_urls: {e}")
            self.logger.log(
                f"Failed data sample: {data_tuples[0] if data_tuples else 'N/A'}"
            )
            self.conn.rollback()
            raise
        except Exception as e:
            self.logger.error(f"Unexpected error during package_urls batch insert: {e}")
            self.conn.rollback()
            raise

    def close(self):
        if self.cursor:
            self.cursor.close()
        if self.conn:
            self.conn.close()
        self.logger.log("PackageUrlsDB connection closed.")


def load_url_id_cache_from_file(
    cache_file_path: str, logger: Logger
) -> Dict[Tuple[str, uuid.UUID], uuid.UUID]:
    """Load URL ID cache from the CSV file generated by batch_insert_urls.py."""
    logger.log(f"Loading url_id cache from file: {cache_file_path}...")
    cache: Dict[Tuple[str, uuid.UUID], uuid.UUID] = {}
    try:
        with open(cache_file_path, "r", newline="", encoding="utf-8") as csvfile:
            reader = csv.reader(csvfile)
            header = next(reader, None)  # Skip header
            if not header or header != ["id", "url", "url_type_id"]:
                logger.error(
                    f"Invalid or missing header in URL cache file: {cache_file_path}. Expected ['id', 'url', 'url_type_id']"  # noqa
                )
                raise ValueError("Invalid URL cache file format")

            for i, row in enumerate(reader):
                if len(row) == 3:
                    try:
                        url_id_str, url_str, url_type_id_str = row[0], row[1], row[2]
                        if url_str and url_type_id_str:  # Ensure no empty strings
                            cache[(url_str, uuid.UUID(url_type_id_str))] = uuid.UUID(
                                url_id_str
                            )
                    except ValueError as ve:
                        logger.warn(
                            f"Invalid UUID in URL cache file at row {i+2}: {row} - {ve}"
                        )
                        continue
                else:
                    logger.warn(
                        f"Skipping malformed row in URL cache file at row {i+2}: {row}"
                    )
        logger.log(f"Loaded {len(cache)} URLs into url_id cache from file.")
        return cache
    except FileNotFoundError:
        logger.error(f"URL cache file not found: {cache_file_path}")
        raise
    except Exception as e:
        logger.error(f"Error loading URL cache file {cache_file_path}: {e}")
        raise


def process_package_url_associations(
    input_csv_path: str,
    batch_size: int,
    script_execution_time: datetime,
    url_cache_csv_path: Optional[str],
    stop_at: Optional[int],
    main_logger: Logger,
) -> None:
    """Main processing logic for associating packages with URLs."""
    main_logger.log(f"Starting package-URL association for: {input_csv_path}")
    main_logger.log(
        f"Batch size: {batch_size}, URL cache: {url_cache_csv_path}, Stop at: {stop_at}"
    )

    try:
        config = Config(PackageManager.NPM)
        url_type_homepage_id = config.url_types.homepage
        url_type_source_id = config.url_types.source
    except Exception as e:
        main_logger.error(f"Error initializing config: {e}")
        return

    db_handler = None
    package_id_cache: Dict[str, uuid.UUID] = {}
    url_id_cache: Dict[Tuple[str, uuid.UUID], uuid.UUID] = {}

    try:
        db_handler = ChaiPackageUrlsDB(main_logger)
        package_id_cache = db_handler.load_package_id_cache()

        if url_cache_csv_path:
            url_id_cache = load_url_id_cache_from_file(url_cache_csv_path, main_logger)
        else:
            main_logger.log(
                "No URL cache file provided, loading all URLs from database..."
            )
            url_id_cache = db_handler.load_url_id_cache_from_db()

    except Exception as e:
        main_logger.error(f"Failed during setup (DB or cache loading): {e}")
        if db_handler:
            db_handler.close()
        return

    package_urls_to_insert: List[
        Tuple[uuid.UUID, uuid.UUID, uuid.UUID, datetime, datetime]
    ] = []
    processed_csv_rows = 0
    total_associations_prepared = 0
    processed_pairs: Set[Tuple[uuid.UUID, uuid.UUID]] = (
        set()
    )  # To avoid duplicates in a single batch

    try:
        with open(input_csv_path, "r", newline="", encoding="utf-8") as infile:
            reader = csv.reader(infile)
            header = next(reader, None)
            if not header:
                main_logger.warn(
                    f"Input CSV file {input_csv_path} is empty or has no header."
                )
                return
            main_logger.log(f"Input CSV Header: {header}")

            for row_num, row in enumerate(reader):
                processed_csv_rows += 1
                current_csv_line = row_num + 2  # 1 for header, 1 for 0-indexing

                if not (len(row) >= 3):
                    main_logger.warn(
                        f"Skipping row {current_csv_line} (length < 3): {row}"
                    )
                    continue

                import_id, source_url_str, homepage_url_str = row[0], row[1], row[2]

                if not import_id:
                    main_logger.warn(
                        f"Skipping row {current_csv_line} due to missing import_id: {row}"  # noqa
                    )
                    continue

                package_id = package_id_cache.get(import_id)
                if not package_id:
                    # We didn't load all the packages from ITN, so this is expected
                    continue

                urls_to_link = []
                if source_url_str and source_url_str.lower() != "null":
                    source_key = (source_url_str.strip(), url_type_source_id)
                    source_url_id = url_id_cache.get(source_key)
                    if source_url_id:
                        urls_to_link.append(source_url_id)
                    else:
                        main_logger.warn(
                            f"Source URL for import_id '{import_id}' not found in URL cache: '{source_url_str}' (row {current_csv_line})"  # noqa
                        )

                if homepage_url_str and homepage_url_str.lower() != "null":
                    homepage_key = (homepage_url_str.strip(), url_type_homepage_id)
                    homepage_url_id = url_id_cache.get(homepage_key)
                    if homepage_url_id:
                        urls_to_link.append(homepage_url_id)
                    else:
                        main_logger.warn(
                            f"Homepage URL for import_id '{import_id}' not found in URL cache: '{homepage_url_str}' (row {current_csv_line})"  # noqa
                        )

                for url_id_to_link in urls_to_link:
                    if (package_id, url_id_to_link) not in processed_pairs:
                        package_urls_to_insert.append(
                            (
                                uuid.uuid4(),
                                package_id,
                                url_id_to_link,
                                script_execution_time,
                                script_execution_time,
                            )
                        )
                        processed_pairs.add((package_id, url_id_to_link))
                        total_associations_prepared += 1

                if len(package_urls_to_insert) >= batch_size:
                    db_handler.batch_insert_package_urls(package_urls_to_insert)
                    package_urls_to_insert = []
                    processed_pairs.clear()  # Clear after batch insert
                    main_logger.log(
                        f"Processed batch. CSV rows: {processed_csv_rows}, Associations: {total_associations_prepared}"  # noqa
                    )

                if stop_at and processed_csv_rows >= stop_at:
                    main_logger.log(f"Reached stop limit of {stop_at} CSV rows.")
                    break

        if package_urls_to_insert:  # Process remaining
            db_handler.batch_insert_package_urls(package_urls_to_insert)
            main_logger.log(
                f"Processed final batch. CSV rows: {processed_csv_rows}, Associations: {total_associations_prepared}"  # noqa
            )

        main_logger.log(
            f"Package-URL association processing complete. Total CSV rows: {processed_csv_rows}. Associations prepared: {total_associations_prepared}."  # noqa
        )

    except FileNotFoundError:
        main_logger.error(f"Input CSV file not found: {input_csv_path}")
    except csv.Error as e:
        main_logger.error(
            f"CSV reading error in {input_csv_path} near line {reader.line_num if 'reader' in locals() else 'unknown'}: {e}"  # noqa
        )
    except psycopg2.Error as e:
        main_logger.error(f"A database error occurred: {e}")
        main_logger.exception()
    except Exception as e:
        main_logger.error(f"An unexpected error occurred: {e}")
        main_logger.exception()
    finally:
        if db_handler:
            db_handler.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Batch insert package-URL relationships from a CSV file."
    )
    parser.add_argument(
        "file_path",
        help="Path to the input CSV file (import_id, source_url, homepage_url).",
    )
    parser.add_argument(
        "--cache",
        metavar="URL_CACHE_CSV_PATH",
        help="Optional path to the CSV file containing URL IDs (output of batch_insert_urls.py).",  # noqa
    )
    parser.add_argument(
        "--batch-size",
        "-b",
        type=int,
        default=DEFAULT_BATCH_SIZE,
        help=f"Number of records to insert per batch (default: {DEFAULT_BATCH_SIZE}).",
    )
    parser.add_argument(
        "--stop",
        "-s",
        type=int,
        help="Optional: stop processing after this many CSV rows.",
    )
    args = parser.parse_args()

    script_start_time = datetime.now()
    logger = Logger("main_pkg_url_assoc_loader")
    logger.log(f"Script started at {script_start_time.isoformat()}")

    process_package_url_associations(
        input_csv_path=args.file_path,
        batch_size=args.batch_size,
        script_execution_time=script_start_time,
        url_cache_csv_path=args.cache,
        stop_at=args.stop,
        main_logger=logger,
    )

    logger.log(
        f"Script finished. Total execution time: {datetime.now() - script_start_time}"
    )
